{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420abdbd-5d1d-49d0-b2ae-bf3ef2c41529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d25c3ed-ab5e-4e1b-9d09-cbc7e09360b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"archive/train_dataframes.xlsx\")\n",
    "df_test = pd.read_excel(\"archive/test_dataframes.xlsx\")\n",
    "\n",
    "\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df.set_index('datetime', inplace=True)\n",
    "df['month'] = df.index.month\n",
    "\n",
    "#lags\n",
    "lag_1, lag_24, lag_168 = [], [], []\n",
    "df['lag_1'] = df['DEMAND'].shift(1)\n",
    "df['lag_24'] = df['DEMAND'].shift(24)\n",
    "df['lag_168'] = df['DEMAND'].shift(168)\n",
    "\n",
    "# mean and std\n",
    "df['roll_mean_6h']   = df['DEMAND'].rolling(window=6).mean()\n",
    "df['roll_std_6h']    = df['DEMAND'].rolling(window=6).std()  #volatility\n",
    "df['roll_mean_24h']  = df['DEMAND'].rolling(window=24).mean()\n",
    "df['roll_std_24h']   = df['DEMAND'].rolling(window=24).std()\n",
    "df['roll_mean_168h'] = df['DEMAND'].rolling(window=168).mean()\n",
    "df['roll_std_168h']  = df['DEMAND'].rolling(window=168).std()\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# circle encoding bc 23 hr might seem farther than 0 but they are only 1 hr apart. model learns that end of cycle connects to start \n",
    "# Hour of day\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hourOfDay'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hourOfDay']/ 24)\n",
    "\n",
    "# Month\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "# Day of week\n",
    "df['day_sin'] = np.sin(2 * np.pi * df['dayOfWeek'] / 7)\n",
    "df['day_cos'] = np.cos(2 * np.pi * df['dayOfWeek'] / 7)\n",
    "\n",
    "\n",
    "\n",
    "data = df[['hour_sin', 'hour_cos','day_sin','day_sin', \n",
    "           'week_X-2', 'week_X-3', 'week_X-4',\n",
    "           'MA_X-4', 'weekend', 'Holiday_ID',\n",
    "           'T2M_toc', 'month_sin','month_cos', 'DEMAND']].copy()\n",
    "\n",
    "training_data_len = int(np.ceil(len(data)*0.95))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd454b-daf8-424d-83d4-b1baa83ca9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13255d48-1583-4d01-a3b5-0ab0177ec24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model(predicts one step)\n",
    "seq_len = 168\n",
    "X_train, Y_train = [],[]\n",
    "for i in range(168, len(scaled_data)):\n",
    "    X_train.append(scaled_data[i-seq_len:i])\n",
    "    Y_train.append(scaled_data[i,-1])\n",
    "X_train = np.array(X_train) #(samples,168,5)\n",
    "Y_train = np.array(Y_train)\n",
    "X_train.shape , Y_train.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926068b4-b764-4f26-83a9-45f3709229e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "# 1st layer\n",
    "model.add(keras.layers.LSTM(64, return_sequences = True, input_shape = (168,14)))\n",
    "# 2nd layer\n",
    "model.add(keras.layers.LSTM(64, return_sequences = False))\n",
    "# 3rd layer\n",
    "model.add(keras.layers.Dense(32, activation = \"relu\"))\n",
    "# 4th layer\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "# final layer\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer = \"adam\",\n",
    "            loss =\"mse\",   #mae treats all points equaly\n",
    "            metrics = [keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "training = model.fit(X_train,Y_train,epochs = 20\n",
    "                    , batch_size = 64 , verbose=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db829a8f-cedb-480e-8c70-cb37743b55c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Prep test data ====\n",
    "# Correctly get the target variable (Y_test) from the last column\n",
    "test_data = scaled_data[training_data_len - seq_len:]\n",
    "X_test, Y_test = [], []\n",
    "\n",
    "for i in range(seq_len, len(test_data)):\n",
    "    X_test.append(test_data[i - seq_len:i])\n",
    "    # FIX: Get Y_test from the last column (-1), not the first (0)\n",
    "    Y_test.append(test_data[i, -1])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "# ==== Predictions ====\n",
    "predictions_scaled = model.predict(X_test)\n",
    "\n",
    "# ==== Inverse transform ====\n",
    "# Create empty arrays with same number of features for inverse scaling\n",
    "Y_test_full = np.zeros((len(Y_test), scaled_data.shape[1]))\n",
    "pred_full = np.zeros((len(predictions_scaled), scaled_data.shape[1]))\n",
    "\n",
    "# FIX: Put target values into the LAST column (-1) for inverse scaling\n",
    "Y_test_full[:, -1] = Y_test\n",
    "pred_full[:, -1] = predictions_scaled[:, 0]\n",
    "\n",
    "# FIX: Reverse scaling and get the result from the LAST column (-1)\n",
    "Y_test_original = scaler.inverse_transform(Y_test_full)[:, -1]\n",
    "predicted_demand = scaler.inverse_transform(pred_full)[:, -1]\n",
    "\n",
    "\n",
    "# ==== Plotting ====\n",
    "train = data[:training_data_len]\n",
    "test = data[training_data_len:]\n",
    "test = test.copy()\n",
    "test['Predictions'] = predicted_demand\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Note: Plotting test['DEMAND'] which is the original, unscaled value\n",
    "plt.plot(test.index, test['DEMAND'], label='Test (actual)', color='green', alpha=0.8)\n",
    "plt.plot(test.index, test['Predictions'], label='Predicted', color='orange')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Demand\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==== Error metric ====\n",
    "# This will now calculate correctly without a divide-by-zero error\n",
    "mape = np.mean(np.abs((Y_test_original - predicted_demand) / Y_test_original)) * 100\n",
    "print(f\"ðŸ“Š MAPE = {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9eb688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq model\n",
    "\n",
    "xtrain_s2s ,ytrain_s2s= [],[]\n",
    "predict_len = 28\n",
    "seq_len = 168\n",
    "for t in range(seq_len,len(scaled_data) - predict_len):\n",
    "    xtrain_s2s.append(scaled_data[t-seq_len:t])\n",
    "    ytrain_s2s.append(scaled_data[t:t + predict_len, -1])\n",
    "\n",
    "xtrain_s2s, ytrain_s2s = np.array(xtrain_s2s), np.array(ytrain_s2s)\n",
    "xtrain_s2s.shape, ytrain_s2s.shape\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046db6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed\n",
    "\n",
    "\n",
    "ytrain_s2s = ytrain_s2s.reshape(ytrain_s2s.shape[0], ytrain_s2s.shape[1], 1) # seq2seq expects 3D shape\n",
    "\n",
    "# Get the shapes from the data\n",
    "input_len = xtrain_s2s.shape[1]\n",
    "num_features = xtrain_s2s.shape[2]\n",
    "output_len = ytrain_s2s.shape[1]\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model_s2s = Sequential()\n",
    "\n",
    "# === Encoder ===\n",
    "# Reads the input sequence and outputs a context vector (the final hidden state).\n",
    "# return_sequences=False because we only need the final output.\n",
    "model_s2s.add(LSTM(64,  input_shape=(input_len, num_features)))\n",
    "\n",
    "# === Bridge ===\n",
    "# Repeats the context vector 'output_len' times to prepare it for the decoder.\n",
    "model_s2s.add(RepeatVector(output_len))\n",
    "\n",
    "# === Decoder ===\n",
    "# Reads the repeated context vector and generates the output sequence.\n",
    "# return_sequences=True because we need an output for each of the 24 future steps.\n",
    "model_s2s.add(LSTM(64,  return_sequences=True))\n",
    "\n",
    "# === Final Output Layer ===\n",
    "# The TimeDistributed layer applies a Dense layer to every single time step of the\n",
    "# decoder's output. This gives us our final 24 predictions.\n",
    "model_s2s.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "\n",
    "# Compile the model (same as before)\n",
    "model_s2s.compile(optimizer='adam', loss='mse')\n",
    "model_s2s.summary()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split your data into 80% for training and 20% for validation\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    xtrain_s2s, ytrain_s2s, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# This callback will stop training if the validation loss\n",
    "# doesn't improve for 5 straight epochs.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Now, update your .fit() call\n",
    "training = model_s2s.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=100,  # Start with a larger number of epochs\n",
    "    batch_size=128,\n",
    "    # === Key Additions ===\n",
    "    validation_data=(X_val, Y_val),\n",
    "    callbacks=[early_stopping],\n",
    "    # =====================\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90554742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your trained Seq2Seq model\n",
    "# model_s2s \n",
    "\n",
    "# Your original scaled data and scaler\n",
    "# scaled_data\n",
    "# scaler\n",
    "\n",
    "# --- 1. Prepare the Test Set ---\n",
    "# We'll use the same logic as your original notebook to define the test set.\n",
    "\n",
    "training_data_len = int(np.ceil(len(data) * 0.95)) # Assuming 'data' is your pre-scaled DataFrame\n",
    "test_data = scaled_data[training_data_len - input_len:]\n",
    "\n",
    "# Create the sliding window for the test set\n",
    "X_test, Y_test_scaled = [], []\n",
    "\n",
    "for i in range(input_len, len(test_data) - output_len):\n",
    "    X_test.append(test_data[i - input_len:i])\n",
    "    Y_test_scaled.append(test_data[i:i + output_len, -1]) # Target is the last column\n",
    "\n",
    "X_test, Y_test_scaled = np.array(X_test), np.array(Y_test_scaled)\n",
    "\n",
    "\n",
    "# --- 2. Predict and Inverse Transform ---\n",
    "\n",
    "# Get the scaled predictions from the model\n",
    "predictions_scaled = model_s2s.predict(X_test)\n",
    "\n",
    "# The scaler expects a 2D array, so we need to reshape our data.\n",
    "# The model outputs (samples, 24, 1), we need (samples * 24, 1) to process.\n",
    "num_samples = X_test.shape[0]\n",
    "num_features = data.shape[1] # Your original number of features (e.g., 14)\n",
    "\n",
    "# Create a placeholder array to hold the predictions for inverse transforming\n",
    "# Shape: (number of total predicted steps, number of original features)\n",
    "pred_placeholder = np.zeros((num_samples * output_len, num_features))\n",
    "\n",
    "# Put the predictions into the last column of the placeholder\n",
    "# We use .flatten() to make the 3D prediction array into a 1D list\n",
    "pred_placeholder[:, -1] = predictions_scaled.flatten()\n",
    "\n",
    "# Now, inverse transform the placeholder\n",
    "predictions_unscaled = scaler.inverse_transform(pred_placeholder)[:, -1]\n",
    "\n",
    "# Reshape back to the same shape as the predictions\n",
    "predictions_unscaled = predictions_unscaled.reshape(num_samples, output_len)\n",
    "\n",
    "# Do the same for the actual test values for comparison\n",
    "actual_placeholder = np.zeros((num_samples * output_len, num_features))\n",
    "actual_placeholder[:, -1] = Y_test_scaled.flatten()\n",
    "Y_test_unscaled = scaler.inverse_transform(actual_placeholder)[:, -1]\n",
    "Y_test_unscaled = Y_test_unscaled.reshape(num_samples, output_len)\n",
    "\n",
    "\n",
    "# --- 3. Plot a Sample Forecast ---\n",
    "\n",
    "# Let's pick a single sample from our test set to visualize the 24-hour forecast\n",
    "sample_index = 500 # You can change this index to see different forecasts\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(Y_test_unscaled[sample_index], label=\"Actual Demand\", marker='.')\n",
    "plt.plot(predictions_unscaled[sample_index], label=\"Predicted Demand\", marker='.')\n",
    "plt.title(f\"24-Hour Demand Forecast (Sample {sample_index})\")\n",
    "plt.xlabel(\"Hours into the Future\")\n",
    "plt.ylabel(\"Demand\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "mape = np.mean(np.abs((Y_test_unscaled - predictions_unscaled) / Y_test_unscaled)) * 100\n",
    "print(f\"ðŸ“Š MAPE = {mape:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
